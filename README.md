# ğŸ¶ JakeLLM â€” Um LLM Multimodal Caseiro em Homenagem ao Jake

<img src="https://github.com/user-attachments/assets/00b83bbf-0695-490e-8380-86ba16254be3" alt="JakeLLM" height="400px" width="900px"/> 



**JakeLLM** Ã© um projeto de modelo de linguagem inspirado nos grandes LLMs como GPT, DeepSeek e LLaMA, mas construÃ­do de forma simples, com recursos limitados e muito coraÃ§Ã£o ğŸ’› â€” em homenagem ao meu pinscher Jake.

---

## âœ¨ Objetivo

Criar um modelo de linguagem **do zero**, em portuguÃªs, treinado localmente ou no Google Colab, com capacidade de inferÃªncia textual e modularidade para crescimento futuro (multimodalidade, geraÃ§Ã£o de imagens, embeddings, etc).

---

## ğŸ“‚ Estrutura do Projeto

```
JakeLLM/
â”œâ”€â”€ model/
â”‚   â””â”€â”€ gpt_model.py # Estrutura do modelo Transformer
â”‚   â””â”€â”€ train.py  # Script de treinamento   
â”œâ”€â”€ tokenizer/
â”‚   â”œâ”€â”€ train_tokenizer.py # Treinamento do tokenizer BPE
â”‚   â””â”€â”€ vocab.json, merges.txt
â”œâ”€â”€ data/raw
â”‚   â””â”€â”€ corpus.txt         # Textos para o tokenizer
â”‚            
â”œâ”€â”€ requirements.txt       # DependÃªncias do projeto
â””â”€â”€ README.md              # Este arquivo
```

---

## ğŸ§  Modelo

O modelo implementa um mini-GPT com:
- ğŸ”¢ **Embedding**: Embeddings de token e posiÃ§Ã£o
- ğŸ” **Camadas**: 4 camadas de Transformer com atenÃ§Ã£o multi-cabeÃ§a
- ğŸ“ **ParÃ¢metros ajustÃ¡veis**: Tamanho do vocabulÃ¡rio, `dim`, nÃºmero de `heads`, `block_size`

---

## ğŸ—£ï¸ Tokenizer

Usamos o `ByteLevelBPETokenizer` da HuggingFace, treinado com um corpus em portuguÃªs contendo:
- Artigos da WikipÃ©dia
- Textos pÃºblicos
- Notas de cÃ³digo

Treinamento com:
```bash
python tokenizer/train_tokenizer.py
```

---

## ğŸ‹ï¸ Treinamento

Script principal: `train.py`

```bash
python train.py
```

O modelo treina em pequenos lotes e imprime o `loss` por Ã©poca.

### Exemplo de resultado:

```
Epoch 1 - Loss: 2.3912
Epoch 2 - Loss: 1.8473
Epoch 3 - Loss: 1.3521
```

---

## ğŸ’» Requisitos

```bash
pip install -r requirements.txt
```

### Principais bibliotecas:
- `torch`
- `transformers`
- `datasets`
- `tokenizers`

---

## ğŸ“Š Exemplo de ExecuÃ§Ã£o

```python
>>> from model.gpt_model import GPT
>>> model = GPT(...)
>>> prompt = "Qual Ã© a capital do Brasil?"
>>> model.generate(prompt)
"BrasÃ­lia Ã© a capital do Brasil."
```

---

## ğŸ–¼ï¸ Homenagem ao Jake

O projeto leva o nome **JakeLLM** em homenagem ao meu cachorro Jake, que sempre esteve ao meu lado nos momentos mais difÃ­ceis. ğŸ¾

---

## ğŸ“Œ Status do Projeto

- âœ… Tokenizer funcional
- âœ… Modelo Transformer bÃ¡sico
- âœ… Treinamento local
- ğŸ”œ Inference em tempo real
- ğŸ”œ Suporte a geraÃ§Ã£o multimodal
- ğŸ”œ Interface web com Gradio

---

## ğŸš€ Meta

Mostrar que Ã© possÃ­vel fazer muito mesmo com poucos recursos â€” basta vontade, estratÃ©gia e paixÃ£o por IA.  
Este projeto Ã© a base de algo muito maior: **um LLM brasileiro, pessoal e original**.

---

## ğŸ“¬ Contato

Feito com â¤ï¸ por [Davi Mattos](https://github.com/DaviMattosDev)
